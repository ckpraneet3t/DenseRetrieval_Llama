# -*- coding: utf-8 -*-
"""Try_llama.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b3I41Wrgs1tK5FW1nSdlhg2Evdw9XsH1
"""



from langchain.document_loaders import PyPDFLoader
from langchain.document_loaders import TextLoader
from langchain.document_loaders import Docx2txtLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from huggingface_hub import notebook_login
import torch
import transformers
from transformers import pipeline
from transformers import AutoTokenizer, AutoModelForCausalLM
from langchain import HuggingFacePipeline
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
import os
import sys

!mkdir docs

document=[]
for file in os.listdir("docs"):
  if file.endswith(".pdf"):
    pdf_path="./docs/"+file
    loader=PyPDFLoader(pdf_path)
    document.extend(loader.load())
  elif file.endswith('.docx') or file.endswith('.doc'):
    doc_path="./docs/"+file
    loader=Docx2txtLoader(doc_path)
    document.extend(loader.load())
  elif file.endswith('.txt'):
    text_path="./docs/"+file
    loader=TextLoader(text_path)
    document.extend(loader.load())

import faiss

# Uses Sentence Transformers Directly

document_splitter = CharacterTextSplitter(separator='\n', chunk_size=500, chunk_overlap=100)
document_chunks = document_splitter.split_documents(document)

from sentence_transformers import SentenceTransformer
embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

document_texts = [doc.page_content for doc in document_chunks]
import numpy as np
document_embeddings = np.array([embedding_model.encode(doc) for doc in document_texts])

index = faiss.IndexFlatL2(document_embeddings.shape[1])
index.add(document_embeddings)
faiss.write_index(index, 'faiss_index.index')

with open('document_chunks.npy', 'wb') as f:
    np.save(f, document_texts)

print("Vector database created and stored successfully.")

# Uses HuggingFaceEmbeddings

document_splitter = CharacterTextSplitter(separator='\n', chunk_size=500, chunk_overlap=100)
document_chunks = document_splitter.split_documents(document)
embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
document_texts = [doc.page_content for doc in document_chunks]
document_embeddings = embeddings.embed_documents(document_texts)
document_embeddings_array = np.array(document_embeddings)

np.save('document_chunks.npy', document_texts)
np.save('document_embeddings.npy', document_embeddings_array)

index = faiss.IndexFlatL2(document_embeddings_array.shape[1])
index.add(document_embeddings_array)
faiss.write_index(index, 'faiss_index.index')

from huggingface_hub import notebook_login
notebook_login()

import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

index = faiss.read_index('faiss_index.index')
with open('document_chunks.npy', 'rb') as f:
    document_texts = np.load(f, allow_pickle=True)

embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base")
pipe = pipeline("text2text-generation", model=model, tokenizer=tokenizer)

def query_knowledge_base(query, top_n=3):
    query_embedding = embedding_model.encode(query).reshape(1, -1)
    distances, indices = index.search(query_embedding, top_n)
    results = sorted([(document_texts[idx], distances[0][i]) for i, idx in enumerate(indices[0])], key=lambda x: x[1])
    print("Top relevant document chunks:")
    for i, (doc, score) in enumerate(results):
        print(f"Rank {i+1}:\nDocument: {doc}\nScore: {score}\n")
    top_documents = " ".join([doc for doc, _ in results])
    prompt = f"Based on the following information:\n{top_documents}\n\ngive a brief answer and rephrase the document text better: {query}"
    result = pipe(prompt, max_length=200)
    return result[0]['generated_text']

if __name__ == "__main__":
    query = input("Enter your query: ")
    answer = query_knowledge_base(query, top_n=3)
    print("Answer:", answer)

embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
vectordb=Chroma.from_documents(document_chunks,embedding=embeddings, persist_directory='./data')
vectordb.persist()

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf",
                                          use_auth_token=True,)


model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf",
                                             device_map='auto',
                                             torch_dtype=torch.float16,
                                             use_auth_token=True,
                                              #load_in_8bit=True,
                                              load_in_4bit=True
                                             )
pipe=pipeline("text-generation",
              model=model,
              tokenizer=tokenizer,
              torch_dtype=torch.bfloat16,
              device_map='auto',
              max_new_tokens=512,
              min_new_tokens=-1,
              top_k=30
              )
llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})
memory=ConversationBufferMemory(memory_key='chat_history', return_messages=True)
pdf_qa=ConversationalRetrievalChain.from_llm(llm=llm,
                                             retriever=vectordb.as_retriever(search_kwargs={'k':6}),
                                             verbose=False, memory=memory)

index = faiss.read_index('faiss_index.index')
with open('document_chunks.npy', 'rb') as f:
    document_texts = np.load(f, allow_pickle=True)


def query_knowledge_base(query, top_n=3):
    query_embedding = embedding_model.encode(query).reshape(1, -1)
    distances, indices = index.search(query_embedding, top_n)
    results = sorted([(document_texts[idx], distances[0][i]) for i, idx in enumerate(indices[0])], key=lambda x: x[1])
    print("Top relevant document chunks:")
    for i, (doc, score) in enumerate(results):
        print(f"Rank {i+1}:\nDocument: {doc}\nScore: {score}\n{'-'*50}")
    top_documents = " ".join([doc for doc, _ in results])
    return top_documents

def query_llama(query, context):
    prompt = f"\n\nBased on the following information:\n{context}\n\nPlease provide a conversational answer to the question and be crisp and on point: {query}"
    result = pipe(prompt, max_new_tokens=200)
    return result[0]['generated_text']

if __name__ == "__main__":
    print("Enter 'exit' to stop the program.")
    while True:
        query = input("Enter your query: ")
        if query.lower() == "exit":
            print("Exiting the program.")
            break

        context = query_knowledge_base(query, top_n=3)
        answer = query_llama(query, context)
#print("Answer:", answer)
        print("\n\033[92mAnswer:\033[0m", answer)

index = faiss.read_index('faiss_index.index')
with open('document_chunks.npy', 'rb') as f:
    document_texts = np.load(f, allow_pickle=True)


def query_knowledge_base(query, top_n=3):
    query_embedding = embedding_model.encode(query).reshape(1, -1)
    distances, indices = index.search(query_embedding, top_n)
    results = sorted([(document_texts[idx], distances[0][i]) for i, idx in enumerate(indices[0])], key=lambda x: x[1])
    print("Top relevant document chunks:")
    for i, (doc, score) in enumerate(results):
        print(f"Rank {i+1}:\nDocument: {doc}\nScore: {score}\n{'-'*50}")
    top_documents = " ".join([doc for doc, _ in results])
    return top_documents

def query_llama(query, context):
    prompt = f"\n\nBased on the following information:\n{context}\n\nPlease provide a conversational answer to the question and be crisp and on point: {query}"
    result = pipe(prompt, max_new_tokens=200)
    return result[0]['generated_text']

if __name__ == "__main__":
    print("Enter 'exit' to stop the program.")
    while True:
        query = input("Enter your query: ")
        if query.lower() == "exit":
            print("Exiting the program.")
            break

        context = query_knowledge_base(query, top_n=3)
        answer = query_llama(query, context)
#print("Answer:", answer)
        print("\n\033[92mAnswer:\033[0m", answer)

result=pdf_qa({"question":"what is supervised learning"})

result['answer']

def query_knowledge_base(query, top_n=3):
    query_embedding = embedding_model.encode(query).reshape(1, -1)
    distances, indices = index.search(query_embedding, top_n)
    results = sorted([(document_texts[idx], distances[0][i]) for i, idx in enumerate(indices[0])], key=lambda x: x[1])
    print("Top relevant document chunks:")
    for i, (doc, score) in enumerate(results):
        print(f"Rank {i+1}:\nDocument: {doc}\nScore: {score}\n")
    top_documents = " ".join([doc for doc, _ in results])
    prompt = f"Based on the following information:\n{top_documents}\n\nPlease provide a detailed and conversational answer to the question,make sense of it and be crisp: {query}"
    result = pipe(prompt, max_length=200)
    return result[0]['generated_text']

if __name__ == "__main__":
    query = input("Enter your query: ")
    answer = query_knowledge_base(query, top_n=3)
    print("Answer:", answer)

